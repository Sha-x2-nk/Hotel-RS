{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "# load dataset\n",
    "df=pd.read_csv(\"Datasets/TripAdvisor.csv\")\n",
    "print(\"################ ORGINAL DATASET LOADED ######################.\")\n",
    "print(df.shape,df.iloc[0,3])\n",
    "print(df.columns)\n",
    "\n",
    "# print(df.head(5)['recommend_list'])\n",
    "print(\"##############################################################.\")\n",
    "\n",
    "\n",
    "# There is a overall rating and 6 class(Value, Location, Sleep Quality, Rooms, Cleanliness, Service) for rating the hotel in recommend_list column\n",
    "# Some users also rated for the Checkin and business service (so we are removing those ratings for consistency among datasets)\n",
    "df=df[~df[\"recommend_list\"].str.contains(\"check\",case=False)]\n",
    "df=df[~df[\"recommend_list\"].str.contains(\"business\",case=False)]\n",
    "\n",
    "\n",
    "# we are removing duplication member,hotel pairs (removing users who have rated the same hotel many times)\n",
    "df.drop_duplicates(subset=[\"member_id\",\"hotel_id\"],inplace=True)\n",
    "\n",
    "\n",
    "# adding extra attributes from recommend_list column as columns in df\n",
    "\n",
    "# Function to extract attributes from 'recommend_list' string\n",
    "def extract_attributes(row):\n",
    "    attributes = {}\n",
    "    recommendations = row['recommend_list'].split(';')\n",
    "    for rec in recommendations:\n",
    "        key, value = rec.split(':')\n",
    "        attributes[value] = float(key)\n",
    "    return pd.Series(attributes)\n",
    "\n",
    "# Apply the function to each row and add new columns to the DataFrame\n",
    "df_attributes = df.apply(extract_attributes, axis=1)\n",
    "df = pd.concat([df, df_attributes], axis=1)\n",
    "\n",
    "\n",
    "# now dropping the unrelevant columns from our dataset\n",
    "df.drop([\"review_id\",\"review_text\", \"recommend_list\"],inplace=True,axis=1)\n",
    "df.to_csv(\"./Datasets/TripAdvisorClean.csv\",index=False)\n",
    "\n",
    "print(\"################ CLEANED DATASET LOADED ######################.\")\n",
    "df = pd.read_csv(\"./Datasets/TripAdvisorClean.csv\")\n",
    "print(df.shape)\n",
    "print(df.head(5))\n",
    "print(\"##############################################################.\")\n",
    "\n",
    "\n",
    "# zeroing nan values\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    print(\"users:\",df[\"member_id\"].value_counts().size)\n",
    "    print(\"Hotels:\",df[\"hotel_id\"].value_counts().size)\n",
    "    print(\"Reviews:\",len(df),'\\n')  \n",
    "    # Max hotels rated by one user\n",
    "    max_hotels_rated = df.groupby('member_id')['hotel_id'].nunique().max()\n",
    "    print(f\"Max hotels rated by one user: {max_hotels_rated}\")\n",
    "\n",
    "    # Min hotels rated by one user\n",
    "    min_hotels_rated = df.groupby('member_id')['hotel_id'].nunique().min()\n",
    "    print(f\"Min hotels rated by one user: {min_hotels_rated}\")\n",
    "\n",
    "    # Avg hotels rated by one user\n",
    "    avg_hotels_rated = df.groupby('member_id')['hotel_id'].nunique().mean()\n",
    "    print(f\"Avg hotels rated by one user: {avg_hotels_rated:.2f}\")\n",
    "\n",
    "\n",
    "    print(f\"hotels rated by one user: {avg_hotels_rated:.2f}\")\n",
    "\n",
    "    # User x avg rating Mat\n",
    "    user_avg_rating = df.groupby('member_id')[['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']].mean()\n",
    "    print(\"\\nUser x avg rating Mat:\")\n",
    "    print(user_avg_rating.head(5))\n",
    "\n",
    "    # Hotel x avg rating\n",
    "    hotel_avg_rating = df.groupby('hotel_id')[['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']].mean()\n",
    "    print(\"Hotel x avg rating:\")\n",
    "    print(hotel_avg_rating.head(5))\n",
    "stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "except:\n",
    "    print(\"PyTorch not found. Install from: https://pytorch.org/get-started/locally/\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, L, activation_fn_type_inner = 'sigmoid', activation_fn_type_outer = 'sigmoid'): \n",
    "        \"\"\"\n",
    "            Class for Symmetrical AutoEncoder Network\n",
    "            params:\n",
    "                L => List of layers\n",
    "                \n",
    "                    for ex: [500, 20, 10] will result in: \n",
    "                    2 Layer Encoder: 500x20, 20x10 -> final representation size = 10\n",
    "                    2 Layer Decoder: 10x20, 20x500 -> input - like\n",
    "        \"\"\"\n",
    "        # parent constuctor\n",
    "        super().__init__()\n",
    "\n",
    "        # gets layer dimensions \n",
    "        # example -> [ [500, 20], [20, 10] ]\n",
    "        self.layers = self.createNnStructure(L)\n",
    "\n",
    "        # getting and saving activation functions\n",
    "        self.inner_activation_fn = self.getActivationFn(activation_fn_type_inner)\n",
    "        self.outer_activation_fn = self.getActivationFn(activation_fn_type_outer)\n",
    "\n",
    "        # create a list of affine forward layers\n",
    "        self.linears = nn.ModuleList([])\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            linear_layer = nn.Linear(layer_size[0], layer_size[1])\n",
    "            # Custom weight initialization, used in paper 2\n",
    "            # nn.init.normal_(linear_layer.weight, mean=0.0, std=0.02)\n",
    "            nn.init.xavier_normal_(linear_layer.weight)\n",
    "            nn.init.constant_(linear_layer.bias, 0.0) # fill bias with 0s\n",
    "            self.linears.append(linear_layer)\n",
    "\n",
    "    def getActivationFn(self, activation_fn_type):\n",
    "        \"\"\"\n",
    "            Input: str, activation function name\n",
    "            Output: respective activation function from torch.nn\n",
    "        \"\"\"\n",
    "        if activation_fn_type == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation_fn_type == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif activation_fn_type == 'elu':\n",
    "            return nn.ELU()\n",
    "        elif activation_fn_type == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation_fn_type == 'softmax':\n",
    "            return nn.Softmax()\n",
    "        elif activation_fn_type == 'linear':\n",
    "            return nn.Identity()\n",
    "        else:\n",
    "            raise ValueError('Undefined activation function used')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass for auto encoder\n",
    "        last_layer_idx = len(self.linears) - 1\n",
    "        out = x.clone()\n",
    "        \n",
    "        # applying inner actviation function for all layers except last one.\n",
    "        for i, layer in enumerate(self.linears):\n",
    "            # apply inner activation function to all layers except last\n",
    "            if i < last_layer_idx:\n",
    "                out = self.inner_activation_fn( layer(out) )\n",
    "        \n",
    "        # applying outer actviation function for last layer\n",
    "        out = self.outer_activation_fn( self.linears[-1](out) )\n",
    "        return out\n",
    "\n",
    "    def createNnStructure(self, L):\n",
    "        # creates nn structure\n",
    "        # for ex: [500, 20, 10] will result in: \n",
    "        #   2 Layer Encoder: 500x20, 20x10 -> final representation size = 10\n",
    "        #   2 Layer Decoder: 10x20, 20x500 -> input - like\n",
    "        # output: [ [500, 20], [20, 10], [10, 20], [20, 500] ]\n",
    "        max_ind = len(L) - 1\n",
    "        layers = []\n",
    "        \n",
    "        # encoder layers\n",
    "        for i, v in enumerate(L):\n",
    "            if i < max_ind:\n",
    "                layer = [v, L[i + 1]]\n",
    "                layers.append(layer)\n",
    "        \n",
    "        # decoder layers\n",
    "        encoder_layers = layers[:]\n",
    "        # reversing encoder layers\n",
    "        for l in encoder_layers[::-1]:\n",
    "            # reversing layer structure as well\n",
    "            decoder_layer = l[::-1]\n",
    "            \n",
    "            layers.append(decoder_layer)\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology. Will train an autoencoder for each rated parameter. First training on overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train test \n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except:\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split\n",
    "First we get random 10% records from out dataset.\n",
    "## train set (overall_rating_train)\n",
    "the basic logic is, all the users, whose records are in test set, should be removed from train set\n",
    "## test set (overall_rating_test, overall_rating_test_labels)\n",
    "there will be 2 test tests. \n",
    "\n",
    "### overall_rating_test \n",
    "One will be the matrix which will be passed to auto encoder, i.e., it will have ratings, but our target ( to test ) ratings will be hidden.\n",
    "<code>\n",
    "\n",
    "ex: [1, 3, 4, 0, 0, 0, 4, 5, 2, 1]\n",
    "\n",
    "                  ^  ^  ^\n",
    "              \n",
    "                to be tested ratings hidden \n",
    "</code>\n",
    "\n",
    "### overall_rating_test_labels\n",
    "One will be the matrix which will be passed to auto encoder, i.e., it will have ratings, but our target ( to test ) ratings will be hidden.\n",
    "\n",
    "<code>\n",
    "ex: [0, 0, 0, 1, 2, 3, 0, 0, 0, 0]\n",
    "\n",
    "                  ^  ^  ^\n",
    "\n",
    "                to be tested ratings \n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class AEMC:\n",
    "    def __init__(self, L, activation_fn_type_inner = 'sigmoid', activation_fn_type_outer = 'sigmoid', metric = 'rating'):\n",
    "        self.model = AutoEncoder(L, activation_fn_type_inner, activation_fn_type_outer)\n",
    "        self.model = self.model.to(device)\n",
    "        self.metric = metric\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train(self, overall_rating_train, batch_size = 100, num_epochs = 200, weight_decay = 0.1, learning_rate = 0.001, optimiser = 'rmsprop', verbose = False):\n",
    "        # Calculate the number of batches and the number of elements in the last batch\n",
    "        num_batches = len(overall_rating_train) // batch_size\n",
    "        remaining_elements = len(overall_rating_train) % batch_size\n",
    "\n",
    "        # Split the array into batches\n",
    "        batches = [overall_rating_train[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "        # Add the smaller last batch if there are remaining elements\n",
    "        if remaining_elements > 0:\n",
    "            batches.append(overall_rating_train[-remaining_elements:])\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        if optimiser == 'rmsprop':\n",
    "            optimiser = torch.optim.RMSprop(self.model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "        elif optimiser == 'adam':\n",
    "            optimiser = torch.optim.Adam(self.model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "        elif optimiser == 'adagrad':\n",
    "            optimiser = torch.optim.Adagrad(self.model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "        elif optimiser == 'sgd':\n",
    "            optimiser = torch.optim.SGD(self.model.parameters(), lr= learning_rate, weight_decay= weight_decay)\n",
    "        else:\n",
    "            print(\"UNDEFINED OPTIMISER\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            for batch in batches:\n",
    "                target = batch.copy()\n",
    "                batch = torch.from_numpy(batch).to(device)\n",
    "\n",
    "                target = torch.from_numpy(target).to(device)\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                output = self.model(batch.clone())\n",
    "                batch.requires_grad = True\n",
    "\n",
    "                # don't consider unrated hotels in loss calculation\n",
    "                output[batch == 0] = 0\n",
    "                loss = criterion(output, target)\n",
    "                train_loss += loss.data\n",
    "\n",
    "                # backpropragation and gradient descent\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"EPOCH: {epoch} loss: {train_loss/len(batches)}\")\n",
    "\n",
    "\n",
    "    def test(self, overall_rating_test, overall_rating_test_labels):\n",
    "        infer_rating_test = self.model( torch.from_numpy(overall_rating_test).to(device) ).to('cpu').detach().numpy()\n",
    "        infer_rating_test[overall_rating_test_labels == 0] = 0\n",
    "\n",
    "        # calculating MSE \n",
    "        error = 0\n",
    "        error_w_avg = 0\n",
    "        count = 0\n",
    "        for  i in range(len(overall_rating_test_labels)):\n",
    "\n",
    "            a = overall_rating_test_labels[i][overall_rating_test_labels[i] != 0]\n",
    "            b = infer_rating_test[i][overall_rating_test_labels[i] != 0]\n",
    "            if(i == 0):\n",
    "                print(f\"EX: \")\n",
    "                print(a)\n",
    "                print(b)\n",
    "            error = error + np.sum(np.square(a - b))\n",
    "            error_w_avg = error_w_avg + np.sum(np.square(a - 2.5))\n",
    "            count += len(a)\n",
    "\n",
    "        error = np.sqrt(error / count)\n",
    "        error_w_avg = np.sqrt(error_w_avg / count)\n",
    "        print(f\"test point count: {count}\")\n",
    "        print(f\"RMSE [w 2.5]: {error_w_avg} | RMSE AE: {error} | difference: {np.abs(error_w_avg - error)} | % improvement {100 * (error_w_avg - error)/error_w_avg}%\")\n",
    "        return 100 * (error_w_avg - error)/error_w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting test data, model inferring from all 4 models and then avg of those ratings\n",
    "def get_total_test_data(df:pd.DataFrame, test_split = 0.05, verbose = False, random_state = 42):\n",
    "\n",
    "    # Filter rows where any of the ratings is 0\n",
    "    # splitting by sklearn split\n",
    "    filtered_df = df[(df['rating'] != 0) & (df['Value'] != 0) & (df['Sleep Quality'] != 0) & (df['Service'] != 0) & (df['Rooms'] != 0) & (df['Location'] != 0) & (df['Cleanliness'] != 0) ]\n",
    "    df_train, df_test_records = train_test_split(filtered_df, test_size= test_split, random_state= random_state)\n",
    "\n",
    "    # removing all records of users who are in set, from train set\n",
    "    df_train = df_train[~df_train['member_id'].isin(df_test_records['member_id'])]\n",
    "    df_test_users = df[df['member_id'].isin(df_test_records['member_id'])]\n",
    "\n",
    "    # assigning indexes instead of member_id and hotel_id\n",
    "    member_id_mapping_train = {member_id: idx for idx, member_id in enumerate(df_train['member_id'].unique())}\n",
    "    hotel_id_mapping = {hotel_id: idx for idx, hotel_id in enumerate(df['hotel_id'].unique())}\n",
    "    \n",
    "    if verbose:\n",
    "        print('####################### MAPPED TRAIN DATA TO INDEXES ###########################.')\n",
    "        print(df_train.head())\n",
    "        print('################################################################################.')\n",
    "\n",
    "    overall_rating_train = {}\n",
    "\n",
    "    # iterating over rows to add records to train set\n",
    "    for metric in ['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']:\n",
    "        overall_rating_train_ = np.zeros( (len(member_id_mapping_train), len(hotel_id_mapping)) ).astype(np.float32)\n",
    "        for _, row in df_train.iterrows():\n",
    "            overall_rating_train_[ member_id_mapping_train[row['member_id']], hotel_id_mapping[row['hotel_id']]] = row[metric]\n",
    "        overall_rating_train[metric] = overall_rating_train_\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"train_data shape: {overall_rating_train.shape}\")\n",
    "\n",
    "    # testing \n",
    "    # assigning indexes instead of member_id and hotel_id\n",
    "    member_id_mapping_test = {member_id: idx for idx, member_id in enumerate(df_test_users['member_id'].unique())}\n",
    "\n",
    "    if verbose:\n",
    "        print('####################### MAPPED TEST DATA TO INDEXES ###########################.')\n",
    "        print(df_test_users.head())\n",
    "        print('###############################################################################.')\n",
    "\n",
    "    overall_rating_test = {}\n",
    "    overall_rating_test_labels = {}\n",
    "\n",
    "    for metric in ['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']:\n",
    "        # creating test set\n",
    "        overall_rating_test_ = np.zeros( (len(member_id_mapping_test), len(hotel_id_mapping)) ).astype(np.float32)\n",
    "\n",
    "        for _, row in df_test_users.iterrows():\n",
    "            overall_rating_test_[ member_id_mapping_test[row['member_id']], hotel_id_mapping[row['hotel_id']]] = row[metric]\n",
    "\n",
    "        # these are actual to be tested entries\n",
    "        overall_rating_test_labels_ = np.zeros( (len(member_id_mapping_test), len(hotel_id_mapping)) ).astype(np.float32)\n",
    "\n",
    "        # removing to-be-tested entries from test set\n",
    "        for _, row in df_test_records.iterrows():\n",
    "            overall_rating_test_[ member_id_mapping_test[row['member_id']], hotel_id_mapping[row['hotel_id']] ] = 0\n",
    "            overall_rating_test_labels_[ member_id_mapping_test[row['member_id']], hotel_id_mapping[row['hotel_id']]] = row[metric]\n",
    "\n",
    "        overall_rating_test[metric] = overall_rating_test_\n",
    "        overall_rating_test_labels[metric] = overall_rating_test_labels_\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Test data shape: {overall_rating_test_.shape}\")\n",
    "\n",
    "    return (overall_rating_train, overall_rating_test, overall_rating_test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acts_in = ['sigmoid', 'tanh', 'softmax']\n",
    "# acts_out = ['elu', 'relu','linear']\n",
    "\n",
    "classes = ['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']\n",
    "\n",
    "overall_rating_train, overall_rating_test, overall_rating_test_labels = get_total_test_data(df, 0.05)\n",
    "\n",
    "\n",
    "best_improve = 0\n",
    "models = {}\n",
    "for param_class in classes:\n",
    "    for sz in [512]:\n",
    "        for optim in ['rmsprop']:\n",
    "            for in_act, out_act in zip(['sigmoid'], ['elu']):\n",
    "                print(f\"{param_class}\\n X_TRAIN DIM: {overall_rating_train[param_class].shape} X_TRAIN NON ZERO VALUES: {(overall_rating_train[param_class]!=0).sum()}\\n X_TEST DIM: {overall_rating_test[param_class].shape} X_TEST NON ZERO VALUES: {(overall_rating_test[param_class]!=0).sum()}\\n Y_TEST DIM: {overall_rating_test_labels[param_class].shape} Y_TEST NON ZERO VALUES: {(overall_rating_test_labels[param_class]!=0).sum()}\")\n",
    "                print(f\"################################# Param Class: {param_class} sz: {sz} actin: {in_act} actout: {out_act} ################################\")\n",
    "\n",
    "                models[param_class] = AEMC(L=[1811, sz], activation_fn_type_inner= in_act, activation_fn_type_outer=out_act, metric=param_class)\n",
    "                models[param_class].train(overall_rating_train[param_class], batch_size= 100, num_epochs= 200, weight_decay= 0.1, learning_rate= 0.001, optimiser= optim, verbose = False)\n",
    "                improvement = models[param_class].test(overall_rating_test[param_class], overall_rating_test_labels[param_class])\n",
    "\n",
    "                if(best_improve < improvement):\n",
    "                    best_improve = improvement\n",
    "                    best = (sz, optim, in_act, out_act)\n",
    "                \n",
    "                print(f\"#############################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_rating_test = np.zeros_like(overall_rating_test['rating'], dtype= np.float32)\n",
    "\n",
    "metrics = ['rating', 'Cleanliness', 'Location', 'Rooms', 'Service', 'Sleep Quality', 'Value']\n",
    "for metric in metrics:\n",
    "    infer_rating_test_ = models[metric]( torch.from_numpy(overall_rating_test[metric]).to(device) ).to('cpu').detach().numpy()\n",
    "    infer_rating_test_[overall_rating_test_labels[metric] == 0] = 0\n",
    "    infer_rating_test += infer_rating_test_\n",
    "\n",
    "infer_rating_test /= len(metrics) # 7 metrics\n",
    "\n",
    "\n",
    "\n",
    "# calculating MSE \n",
    "error = 0\n",
    "error_w_avg = 0\n",
    "count = 0\n",
    "for  i in range(len(overall_rating_test_labels['rating'])):\n",
    "\n",
    "    a = overall_rating_test_labels['rating'][i][overall_rating_test_labels['rating'][i] != 0]\n",
    "    b = infer_rating_test[i][overall_rating_test_labels['rating'][i] != 0]\n",
    "    if(i == 0):\n",
    "        print(f\"EX: \")\n",
    "        print(a)\n",
    "        print(b)\n",
    "    error = error + np.sum(np.square(a - b))\n",
    "    error_w_avg = error_w_avg + np.sum(np.square(a - 2.5))\n",
    "    count += len(a)\n",
    "\n",
    "error = np.sqrt(error / count)\n",
    "error_w_avg = np.sqrt(error_w_avg / count)\n",
    "print(f\"test point count: {count}\")\n",
    "print(f\"RMSE [w 2.5]: {error_w_avg} | RMSE AE: {error} | difference: {np.abs(error_w_avg - error)} | % improvement {100 * (error_w_avg - error)/error_w_avg}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
